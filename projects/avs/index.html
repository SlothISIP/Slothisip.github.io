<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Acoustic Vision System (AVS) | Ju O Kim</title>

    <meta name="description" content="Multimodal AI acoustic camera platform with competitive results on DCASE 2020 Task 2 benchmark for industrial anomaly detection.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Ju O Kim">
    <link rel="canonical" href="https://slothisip.github.io/projects/avs/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://slothisip.github.io/projects/avs/">
    <meta property="og:title" content="Acoustic Vision System - Industrial Anomaly Detection">
    <meta property="og:description" content="Multimodal AI acoustic camera with competitive DCASE 2020 Task 2 benchmark results.">
    <meta property="og:image" content="https://slothisip.github.io/images/avs_thumb.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:site_name" content="Ju O Kim - Portfolio">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://slothisip.github.io/projects/avs/">
    <meta name="twitter:title" content="Acoustic Vision System - Industrial Anomaly Detection">
    <meta name="twitter:description" content="Multimodal AI acoustic camera with competitive DCASE 2020 benchmark results.">
    <meta name="twitter:image" content="https://slothisip.github.io/images/avs_thumb.png">
    <meta name="twitter:image:alt" content="AVS beamforming and acoustic localization">

    <link rel="shortcut icon" href="../../images/favicon/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                AI-Powered Acoustic Vision System (AVS)
                <small>
                    Industrial Anomaly Detection | DCASE 2020 Benchmark
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="../../index.html">
                          Ju O Kim
                        </a>
                        </br>Keimyung University
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <p class="text-justify">
                    I designed and built AVS as a solo developer handling both backend and frontend end-to-end, with guidance from my team manager. This multimodal AI platform combines acoustic imaging, computer vision, and deep learning for industrial machine anomaly detection. I evaluated the system on the DCASE 2020 Task 2 benchmark, achieving competitive results across multiple machine types (Fan, Pump, Slider, Valve), with comprehensive test coverage across 1,091 automated tests.
                </p>
                <div style="background: #f8f9fa; border-left: 4px solid #007bff; padding: 15px; margin: 15px 0;">
                    <p style="margin: 0;"><strong>Note:</strong> This project was developed for industrial clients under NDA. Specific client names and proprietary datasets have been anonymized. The methodology and architecture described here accurately represent my contributions.</p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    My Contributions
                </h3>
                <ul>
                    <li><strong>System Architecture:</strong> I designed the complete acoustic vision pipeline from microphone array input through MVDR beamforming to deep learning inference</li>
                    <li><strong>Deep Learning:</strong> I implemented and fine-tuned the Audio Spectrogram Transformer (AST) with LoRA adaptation, reducing parameters from 86M to 2.5M while maintaining performance</li>
                    <li><strong>Backend Development:</strong> I built the entire audio processing pipeline using torchaudio and librosa, including STFT spectrogram processing</li>
                    <li><strong>Frontend Development:</strong> I developed the real-time visualization dashboard for beamforming heatmaps and anomaly detection results</li>
                    <li><strong>Benchmark Evaluation:</strong> I conducted comprehensive evaluation on DCASE 2020 Task 2, targeting 20-30% reduction in unplanned factory downtime</li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    System Architecture
                </h3>
                <center>
                    <img src="images/architecture.svg" class="img-responsive" alt="System Architecture" style="max-width: 100%; background: #0a192f; border-radius: 8px; padding: 10px;">
                </center>
                <br>
                <p class="text-justify">
                    I designed this acoustic vision pipeline from microphone array hardware through MVDR beamforming for sound source localization, STFT spectrogram processing, to Audio Spectrogram Transformer (AST) with LoRA adaptation. I implemented the fusion of acoustic features with YOLOv8 visual detection and evaluated the system on DCASE 2020 Task 2 benchmark.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Beamforming Heatmap Visualization
                </h3>
                <center>
                    <img src="images/AVS_1.png" class="img-responsive" alt="Beamforming Heatmap Visualization">
                </center>
                <br>
                <p class="text-justify">
                    Spatial acoustic intensity heatmap generated by the beamforming algorithm. The visualization shows 2D sound source localization results overlaid on the camera view, enabling precise identification of acoustic events in monitored environments using MVDR (Minimum Variance Distortionless Response) beamforming.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    DCASE Benchmark Results
                </h3>
                <center>
                    <img src="images/AVS_2.png" class="img-responsive" alt="DCASE Benchmark Results">
                </center>
                <br>
                <p class="text-justify">
                    Benchmark evaluation on the DCASE 2020 Task 2 challenge dataset for unsupervised anomalous sound detection. The AST+LoRA approach demonstrates competitive performance across 4 machine types (Fan, Pump, Slider, Valve), targeting industrial predictive maintenance applications with 20-30% downtime reduction potential.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Time-Frequency Spectrogram Analysis
                </h3>
                <center>
                    <img src="images/AVS_3.png" class="img-responsive" alt="Time-Frequency Spectrogram Analysis">
                </center>
                <br>
                <p class="text-justify">
                    Detailed time-frequency spectrogram analysis showing acoustic signatures of normal and anomalous machine operations. The STFT visualization highlights distinctive frequency patterns in the 2000-4000 Hz range that enable deep learning models to detect subtle anomalies in industrial equipment.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Audio Spectrogram Transformer Architecture
                </h3>
                <center>
                    <img src="images/AVS_4.png" class="img-responsive" alt="AST Architecture">
                </center>
                <br>
                <p class="text-justify">
                    Architecture diagram of the Audio Spectrogram Transformer (AST) model with LoRA adaptation layers. The transformer-based architecture processes mel-spectrogram patches through self-attention mechanisms, achieving superior feature extraction for acoustic anomaly detection with 12ms per sample inference time on RTX 4090.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Microphone Array Calibration Setup
                </h3>
                <center>
                    <img src="images/AVS_5.png" class="img-responsive" alt="Microphone Array Calibration Setup">
                </center>
                <br>
                <p class="text-justify">
                    Hardware calibration setup for the microphone array system. The visualization shows precise geometric arrangement of microphones and the calibration procedure ensuring accurate beamforming and sound source localization performance.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Key Technical Decisions
                </h3>
                <table class="table table-bordered" style="font-size: 0.9em;">
                    <tbody>
                        <tr>
                            <td style="width: 25%;"><strong>Decision</strong></td>
                            <td><strong>AST + LoRA over Full Fine-tuning</strong></td>
                        </tr>
                        <tr>
                            <td><strong>Context</strong></td>
                            <td>Limited anomaly training data (few hundred samples per machine type); full fine-tuning leads to overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Options</strong></td>
                            <td>Train from scratch, Full fine-tuning, LoRA adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>Rationale</strong></td>
                            <td>LoRA reduces trainable parameters from 86M to 2.5M (97% reduction) while maintaining 99.2% of full fine-tuning performance; prevents overfitting on small anomaly datasets</td>
                        </tr>
                    </tbody>
                </table>
                <table class="table table-bordered" style="font-size: 0.9em;">
                    <tbody>
                        <tr>
                            <td style="width: 25%;"><strong>Decision</strong></td>
                            <td><strong>MVDR over Delay-and-Sum Beamforming</strong></td>
                        </tr>
                        <tr>
                            <td><strong>Context</strong></td>
                            <td>Factory environments have multiple noise sources and reverberations</td>
                        </tr>
                        <tr>
                            <td><strong>Options</strong></td>
                            <td>Delay-and-Sum (DAS), MVDR, MUSIC</td>
                        </tr>
                        <tr>
                            <td><strong>Rationale</strong></td>
                            <td>MVDR provides adaptive null steering to suppress interfering noise sources while maintaining signal-of-interest; 8dB SNR improvement over DAS in factory conditions</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Technical Specifications
                </h3>
                <table class="table table-bordered">
                    <tbody>
                        <tr>
                            <td><strong>Benchmark</strong></td>
                            <td>DCASE 2020 Task 2 (Competitive results)</td>
                        </tr>
                        <tr>
                            <td><strong>Inference Speed</strong></td>
                            <td>12ms/sample on RTX 4090</td>
                        </tr>
                        <tr>
                            <td><strong>Codebase</strong></td>
                            <td>Full-stack implementation across 160+ modules</td>
                        </tr>
                        <tr>
                            <td><strong>Test Suite</strong></td>
                            <td>1,091 automated tests</td>
                        </tr>
                        <tr>
                            <td><strong>Deep Learning</strong></td>
                            <td>PyTorch 2.0+, AST, LoRA, YOLOv8</td>
                        </tr>
                        <tr>
                            <td><strong>Audio Processing</strong></td>
                            <td>torchaudio, librosa, MVDR beamforming</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

    </div>
</body>
</html>
